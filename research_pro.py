import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
from collections import Counter
import re
import matplotlib as mpl
from wordcloud import WordCloud
from io import BytesIO
from docx import Document
import networkx as nx 
from langdetect import detect
import nltk
from nltk.corpus import cmudict # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ô‡∏±‡∏ö‡∏û‡∏¢‡∏≤‡∏á‡∏Ñ‡πå‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©

# ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö NLTK
try:
    nltk.data.find('corpora/cmudict')
except LookupError:
    nltk.download('cmudict')
    nltk.download('averaged_perceptron_tagger')
    nltk.download('universal_tagset')

d = cmudict.dict()

# --- ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏±‡∏ö‡∏û‡∏¢‡∏≤‡∏á‡∏Ñ‡πå‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏© ---
def count_syllables_en(word):
    word = word.lower()
    if word in d:
        return max([len([list(y for y in x if y[-1].isdigit()) for x in d[word]][0])])
    # ‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÉ‡∏ô‡∏î‡∏¥‡∏Å‡∏ä‡∏±‡∏ô‡∏ô‡∏≤‡∏£‡∏µ ‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏ô‡∏±‡∏ö‡∏™‡∏£‡∏∞‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
    return len(re.findall(r'[aeiouy]+', word))

# --- ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏±‡∏ö‡∏û‡∏¢‡∏≤‡∏á‡∏Ñ‡πå‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ---
def count_syllables_th(word):
    try:
        from pythainlp.tokenize import syllable_tokenize
        return len(syllable_tokenize(word))
    except:
        return 0

# --- 1. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ---
font_path = "Kanit-Regular.ttf" 

def setup_font():
    try:
        mpl.font_manager.fontManager.addfont(font_path)
        prop = mpl.font_manager.FontProperties(fname=font_path)
        mpl.rc('font', family=prop.get_name(), size=12)
        mpl.rcParams['axes.unicode_minus'] = False 
        return prop
    except:
        return None

# --- 2. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ä‡πà‡∏ß‡∏¢‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå Export ---
def to_excel(df):
    output = BytesIO()
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        df.to_excel(writer, index=False, sheet_name='Summary')
    return output.getvalue()

# --- 3. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå ---
def analyze_sentiment(text, lang):
    if lang == 'th':
        pos = ['‡∏î‡∏µ', '‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à', '‡∏†‡∏π‡∏°‡∏¥‡πÉ‡∏à', '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∏‡∏Ç', '‡∏û‡∏±‡∏í‡∏ô‡∏≤', '‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå', '‡∏¢‡∏±‡πà‡∏á‡∏¢‡∏∑‡∏ô', '‡∏û‡∏≠‡πÄ‡∏û‡∏µ‡∏¢‡∏á']
        neg = ['‡πÑ‡∏°‡πà‡∏î‡∏µ', '‡∏õ‡∏±‡∏ç‡∏´‡∏≤', '‡πÅ‡∏¢‡πà', '‡∏¢‡∏≤‡∏Å‡∏•‡∏≥‡∏ö‡∏≤‡∏Å', '‡∏Ç‡∏≤‡∏î‡πÅ‡∏Ñ‡∏•‡∏ô', '‡∏≠‡∏∏‡∏õ‡∏™‡∏£‡∏£‡∏Ñ', '‡∏´‡∏ô‡∏µ‡πâ‡∏™‡∏¥‡∏ô']
    else:
        pos = ['good', 'great', 'excellent', 'success', 'happy', 'positive', 'improve', 'benefit']
        neg = ['bad', 'problem', 'difficult', 'lack', 'obstacle', 'debt', 'negative', 'poor']
    
    pos_score = sum(1 for w in pos if w in text.lower())
    neg_score = sum(1 for w in neg if w in text.lower())
    return "‡∏ö‡∏ß‡∏Å üòä" if pos_score > neg_score else ("‡∏•‡∏ö üòü" if neg_score > pos_score else "‡∏õ‡∏Å‡∏ï‡∏¥ üòê")

# --- ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏° ---
try:
    from pythainlp.tokenize import word_tokenize
    from pythainlp.corpus import thai_stopwords
    from pythainlp.summarize import summarize
    THAI_READY = True
except:
    THAI_READY = False

st.set_page_config(layout="wide", page_title="Syllable Filter Research Tool")
st.title("üï∏Ô∏è ‡∏£‡∏∞‡∏ö‡∏ö‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏á‡∏≤‡∏ô‡∏ß‡∏¥‡∏à‡∏±‡∏¢ (‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡∏≥ 5-10 ‡∏û‡∏¢‡∏≤‡∏á‡∏Ñ‡πå)")

if not THAI_READY:
    st.error("‚ùå Library ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")
    st.stop()

font_p = setup_font()
uploaded_files = st.file_uploader("‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå (.txt)", type=['txt'], accept_multiple_files=True)

if uploaded_files:
    for file in uploaded_files:
        raw_text = file.read().decode("utf-8")
        try: lang = detect(raw_text)
        except: lang = 'th'
        
        # --- ‡∏£‡∏∞‡∏ö‡∏ö‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≤‡∏°‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡πÉ‡∏´‡∏°‡πà ---
        if lang == 'th':
            tokens = word_tokenize(raw_text, keep_whitespace=False)
            stop_words = list(thai_stopwords())
            filtered = []
            for t in tokens:
                t = t.strip()
                # ‡∏ï‡∏±‡∏î‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå/‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏™‡∏±‡πâ‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
                if t and not re.match(r'^[0-9\W]+$', t) and t not in stop_words:
                    syl_count = count_syllables_th(t)
                    if 5 <= syl_count <= 10: # ‡∏Å‡∏£‡∏≠‡∏á‡∏û‡∏¢‡∏≤‡∏á‡∏Ñ‡πå 5-10
                        filtered.append(t)
        else:
            # ‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©: ‡∏ï‡∏±‡∏î Preposition, Article, Conjunction, Number
            import nltk
            words_only = re.findall(r'\b[a-zA-Z]+\b', raw_text.lower())
            tagged = nltk.pos_tag(words_only)
            # ‡∏ï‡∏±‡∏î‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏° (DT), ‡∏Ñ‡∏≥‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏° (CC), ‡∏ö‡∏∏‡∏û‡∏ö‡∏ó (IN)
            excluded_tags = ['DT', 'CC', 'IN', 'PRP', 'PRP$', 'TO', 'MD']
            filtered = []
            for word, tag in tagged:
                if tag not in excluded_tags and len(word) > 2:
                    syl_count = count_syllables_en(word)
                    if 5 <= syl_count <= 10: # ‡∏Å‡∏£‡∏≠‡∏á‡∏û‡∏¢‡∏≤‡∏á‡∏Ñ‡πå 5-10
                        filtered.append(word)

        word_counts = Counter(filtered)
        filtered_final = [w for w in filtered if word_counts[w] >= 1] # ‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ‡πÇ‡∏ä‡∏ß‡πå‡πÅ‡∏°‡πâ‡πÄ‡∏à‡∏≠‡πÅ‡∏Ñ‡πà‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß

        with st.expander(f"üìë ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå: {file.name} ({'‡πÑ‡∏ó‡∏¢' if lang=='th' else 'EN'})", expanded=True):
            if not filtered_final:
                st.warning("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß 5-10 ‡∏û‡∏¢‡∏≤‡∏á‡∏Ñ‡πå‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ")
            else:
                c1, c2 = st.columns(2)
                with c1:
                    st.write(f"**‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á:** {len(filtered_final)} ‡∏Ñ‡∏≥")
                    wc = WordCloud(width=800, height=400, background_color="white", font_path=font_path).generate(" ".join(filtered_final))
                    fig, ax = plt.subplots()
                    ax.imshow(wc)
                    ax.axis("off")
                    st.pyplot(fig)
                with c2:
                    st.subheader("üìà ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Ñ‡∏≥ (5-10 ‡∏û‡∏¢‡∏≤‡∏á‡∏Ñ‡πå)")
                    df_counts = pd.DataFrame(word_counts.most_common(15), columns=['‡∏Ñ‡∏≥', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á'])
                    st.table(df_counts)
                    st.download_button("üü¢ ‡πÇ‡∏´‡∏•‡∏î Excel", to_excel(df_counts), f"filter_{file.name}.xlsx")
